---
title: "Determinants of crime - report"
author: "Joanna Ceglinska, Szymon Groszkiewicz"
date: "May, 2021"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: true
      smooth_scroll: true
      scroll_highlight: true
    number_sections: false
    theme: united
    highlight: zenburn
    
---
<!-- theme: united, darkly, cerulean, paper, simplex-->

<!-- highlight: zenburn, breezedark, haddock  -->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F, cache = T)

library(aod)
library(bestNormalize)
library(data.table)
library(dplyr)
library(kableExtra)
library(knitr)
library(lmtest)
library(GGally)
library(ggplot2)
library(gplots)
library(plm)
library(stargazer)
library(tseries)
```

```{r, echo=F}
data <- read.csv("Data/data_no_missings.csv", header = T)

data$X <- NULL

# Shortening names of the columns
colnames(data) <- c("Country", "Year", "Homicide", "Inequality", "Education_years", "GDP_per_capita",
                    "Lower_secondary_completion_rate", "RnD_expenditure", "School_enrollment", 
                    "Unemployment", "Urbanization_rate","Unsentenced", "Police")

# Data transformation
df2 <- data %>%
  select(Country, Year, Homicide, Inequality, Education_years, 
         GDP_per_capita, Lower_secondary_completion_rate, 
         School_enrollment, Unemployment, Unsentenced, Police)

df2$ln_Homicide <- log(df2$Homicide)
df2$ln_GDP_per_capita <- log(df2$GDP_per_capita)
df2$Unemployment_int <- cut(df2$Unemployment,
                           breaks = c(0,5.5,8.5,Inf),
                           labels = c("_low", "_medium","_high"))

df3 <- data %>%
  select(Country, Year, Homicide, Inequality, Education_years, 
         GDP_per_capita, Lower_secondary_completion_rate, 
         School_enrollment, Unemployment, Unsentenced, Police,
         Urbanization_rate, RnD_expenditure)

df3$ln_Homicide <- log(df3$Homicide)
df3$ln_GDP_per_capita <- log(df3$GDP_per_capita)
df3$Unemployment_int <- cut(df3$Unemployment,
                            breaks = c(0,5.5,8.5,Inf),
                            labels = c("_low", "_medium","_high"))
df3$ln_RnD_expenditure <- log(df3$RnD_expenditure)
```

# Introduction

The aim of this project is to reproduce the results of a bachelor thesis study prepared in 2019 by Joanna Ceglinska (one of the authors) and extend the research by using different software, bigger dataset and by implementing more advanced methods of data analysis and visualization. 
The topic of the original study was investigating the determinants of crime using panel data of 48 countries for the years 2003-2015. The dependent variable was the homicide rate and independent variables included the Gini index, GDP per capita, unemployment rate, variables responsible for measuring the education level in a country and a regressor which presents the effectiveness of judicial system (Unsentenced detainees as a proportion of overall prison population).


---

# An empirical research

### Main implemented modifications

We have decided to not only replicate the previous research, but also extend it and perform the analysis using different software. Previously, the main tools in the study were Microsoft Excel (for the data tranformations and filling the missing observations) and STATA for statistical and econometric research. Now, we have used R and Python for the regressions and data visualisation.

We have also implemented a new method of missing data imputation - previous work included manual imputation of all missing values, whereas now we use an automated function that have performed this task. The full description of the algorithm is presented in the subsequent section of the report. 

In our research, we extended the dataset by two years and two variables. The original dataset included the years 2003-2015 and we appended the years 2016 and 2017 to the database. Additional variables were the research and development expenditures and the urbanization rate. 


### The dependent variable

The variable responsible for the represenation of crime is the **homicide rate**, which is the number of deaths caused by another person's violence (per 100,000 population). It is worth noting, that a homicide is not equivalent to murder - it does not have to result from the intention of causing harm - it can also be a consequence of an accident or reckless acts.

This variable was chosen because is a serious violent crime, that causes a significant harm to other people. It is one of the few crimes that is carefully tracked and is not neglected. The investigation associated with this type of crime is usually careful and meticulous. What is more, using a variable associated with a bribe or a financial fraud would be more complicated due to the differences in definitions of those crimes and law regulations among countries.

The distribution of this variable is presented in the plot below. The distribution is highly asymmetrical and skewed towards the left side of the plot - the majority of observations are concentrated around zero. Therefore, we have decided to use a logarithmic transformation of homicide rate in the research. 


```{r, echo=F, fig.align='center', out.width = '80%'}
ggplot(data) +
  geom_histogram(aes(Homicide), fill = "Steelblue", color = "black")
```

The next plot presents how the heterogeneity of the homicide rate has been changing in the presented years 2003-2017. It can be observed, that the heterogeneity decreases with time (from 8.5 to around 6), which means that countries tend to differ less in the homicide rate across the world.


```{r , echo=F, fig.align='center', out.width = '120%'}
plotmeans(Homicide ~ Year, main="Heterogeineity across years", data=data)
```

### The dataset description

The variables used in the research are presented in the table below. In the second column we show the source of the data, and the last column presents a Worlbank code to quickly find those variables in the database. We choice of determinants was based on the literature review.

Researchers believe, that crime is dependent on the efficiency of the jurisdictional system, the income inequality in a country, an education level, unemployment and the economic condition of a country. Additional variables refer to the society's approach to innovation and education (Pesearch and development expenditure variable) and the urbanization, because more crimes tend to happen in cities than in the rural areas (Percentage of urban population). 

Table 1. The variables used in the research, their source and additional information if available).

```{r, echo = FALSE}
Variable <- c("Homicide rate (per 100,000 population)",
"Unsentenced detainees as a proportion of overall prison population",
"Police personnel (per 100,000 population)",
"Gini index - income inequality measure",
"Lower secondary completion rate, total (% of relevant age group)",
"School enrollment,  tertiary (% gross)",
"Unemployment, total (% of total labor force) (national estimate)",
"Compulsory education,  duration (years)",
"GDP per capita (constant 2010 US$)",
"Research and development expenditure (% of GDP)",
"Urban population (% of total population)")

Source <- c("United Nations Office on Drugs and Crime (UNODC)", 
            "United Nations Office on Drugs and Crime (UNODC)", 
            "United Nations Office on Drugs and Crime (UNODC)",
            "Standardized World Income Inequality Database (SWIID)",
            "Worldbank",
            "Worldbank",
            "Worldbank",
            "Worldbank",
            "Worldbank",
            "Worldbank",
            "Worldbank"
            )

Info <- c("", 
          "", 
          "",
          "",
          "SE.SEC.CMPT.LO.ZS",
          "SE.TER.ENRR",
          "SL.UEM.TOTL.NE.ZS",
          "SE.COM.DURS",
          "NY.GDP.PCAP.KD",
          "GB.XPD.RSDV.GD.ZS",
          "SP.URB.TOTL.IN.ZS"
          )

df_desc <- data.frame(Variable, Source, Info)

# knitr::kable(df_desc, col.names = gsub("[.]", " ", names(df_desc)))

df_desc %>%
  kbl() %>%
  kable_styling()


```

The original dataset included 48 countries and years 2003-2015, whereas the extended dataset also includes years 2016 and 2017 (for the same set of countries). Adding two years and two variables resulted in obtaining 96 observations more than in the base dataset. The summary of the differences can be found in the Table 2. 

Table 2. The summary of the differences between the datasets

```{r, echo = FALSE}
Original <- c("Years  2003-2015",
              "8 independent variables",
              "624 observations in total")

Extended <- c("Years 2003-2017", 
            "Two additional variables (10 in total): 
              <br> 1. Research and development expenditure 
              <br> 2. Percentage of urban population in total",
            "720 observations in total")

df_differences <- data.frame(Original, Extended)

# knitr::kable(df_differences, col.names = gsub("[.]", " ", names(df_differences)))

df_differences %>%
  kable("html", escape = FALSE) %>%
  kable_styling() 

```

### Filling the missing values




### Research hypotheses

The literature review prompts us to formulate hypotheses as to the influence of selected variables on the dependent variable, which will be verified in an empirical study. The hypotheses posed in the bachelor's thesis on which we base the project are as follows:

* Income inequality has a negative effect on the homicide rate.
* GDP per capita has a negative effect on the homicide rate.
* The level of education in a country influences crime (the higher the level, the lower the crime).
* The ease of detecting a crime acts as a deterrent to the decision to commit a crime.
* Poor labor market conditions in the country have a positive effect on the homicide rate.
* The effectiveness of the justice system has a positive effect on the homicide rate.


# Reproduction

Info about this part, what we are doing, what assumptions etc.

Let us now move to the reproduction modeling part of our project. Based on the same data as in the bachelor's thesis (the one before dealing with the missing values), we check whether we get the same results. For this, However, this time we use for this analysis different software, and therefore slightly different functions. In addition, as already mentioned before, we use a more accurate method of filling in the missing data.

We first decided to briefly present how in general we deal with choosing the appropriate type of model in the case of panel data.

#### The general schema of the type of model selection in the panel data

```{r , echo=FALSE, out.width = '45%', fig.align='center'}
knitr::include_graphics("pictures/model_selection_schema.png")
```

SOURCE ???

At the beginning, we check whether the random and fixed effects are significant based on Breusch-Pagan Lagrange Multiplier Test. If they both are not, we simply use Pooled Ordinary Least Squares (POLS) model. However, if those effects are significant, we then check using Huasmann test which model it is better to use: fixed effects model or random effects model.

In our analysis we also decided to check additionally whether time effects are needed in the model (again using the appropriate Breusch-Pagan Lagrange Multiplier Test). Also in general, the significance level, naturally can be chosen different than 5% as on the graph. Setting this level will be possible, among others, in the functions that are an automated part of our analysis.

## Data transormation

Having filled in the missing values in the data, for reproduction purposes, we limit the data to the variables that were considered in the bachelor's thesis (i.e. without *RnD_expenditure* and *Urbanization_rate*) for the years up to 2015.

```{r}
df <- data %>%
  select(Country, Year, Homicide, Inequality, Education_years, GDP_per_capita,
         Lower_secondary_completion_rate, School_enrollment, Unemployment, Unsentenced, Police) %>%
  filter(Year %in% 2003:2015)
```

Let us perform some data transformation now.

### Homicide

As already shown before, *Homecide* variable seems to be right-skewed. Therefore we should consider its log-transformation. Since there is  `r sum(df$Homicide <= 0)` observations for which *Homicide* is non-positive, we do not need to add any number to it inside the logarithm function.

### {.tabset .tabset-fade .tabset-pills}

#### Basic form

```{r, echo=F, fig.align='center', out.width = '80%'}
ggplot(df) +
  geom_histogram(aes(Homicide), fill = "Steelblue", color = "black")
```

#### Log-transformed

```{r, echo=F, fig.align='center', out.width = '80%'}
ggplot(df) +
  geom_histogram(aes(log(Homicide)), fill = "Steelblue", color = "black")
```

### {-}

Having applied Box-Cox transformation for this variable, we got the result that the optimal $\lambda$ = `r bestNormalize::boxcox(df$Homicide)$lambda %>% round(4)` 

It is not that much close to zero, but since after this transformation the distribution is more symmetric and and since it was performed in the original analysis, we decided to log-transform it.

Therefore, we create the logarithm of the dependent variable:

```{r}
df$ln_Homicide <- log(df$Homicide)
```

### GDP per capita

Now we can consider *GDP_per_capita* variable, which based on the pltos below, seems to have much right-skewed distribution.

### {.tabset .tabset-fade .tabset-pills}

#### Basic form

```{r, echo=F, fig.align='center', out.width = '80%'}
ggplot(df) +
  geom_histogram(aes(GDP_per_capita), fill = "Steelblue", color = "black")
```

#### Log-transformed

```{r, echo=F, fig.align='center', out.width = '80%'}
ggplot(df) +
  geom_histogram(aes(log(GDP_per_capita)), fill = "Steelblue", color = "black")
```

### {-}

Box-Cox transformation for this variable indicates that the optimal $\lambda$ = `r bestNormalize::boxcox(df$Homicide)$lambda %>% round(4)`, which confirms that it is better to use the logarithm of this variable in the model. The same transformation was made in the original analysis.

```{r}
df$ln_GDP_per_capita <- log(df$GDP_per_capita)
```

### Unemployment

In the case of *Unemployment* variable, we decided to transform it exactly in the same way it was done in the bachelor thesis, i.e. to divide its values into 3 groups referred to `low`, `medium` and `high` rate of unemployment.

```{r}
df$Unemployment_int <- cut(df$Unemployment,
                           breaks = c(0,5.5,8.5,Inf),
                           labels = c("_low", "_medium","_high"))
```

## Panel data models

In this part we finally estimate three kinds of model for the panel data, that is fixed effects model, random effects model and POLS model. To do this, we use `plm()` function from the package of the same name. 

In order to obtain the appropriate models, we need to choose the corresponding value of the `model` argument as follows:

```{r}
# fixed effects model
fixed <- plm(ln_Homicide ~ Inequality + Education_years + ln_GDP_per_capita +
             Lower_secondary_completion_rate + School_enrollment + Unemployment_int + Unsentenced + Police,
             data = df, 
             index = c("Country", "Year"),
             model = "within")
```

```{r}
# random effects model
random <- plm(ln_Homicide ~ Inequality + Education_years + ln_GDP_per_capita +
               Lower_secondary_completion_rate + School_enrollment + Unemployment_int + Unsentenced + Police,
             data = df, 
             index = c("Country", "Year"),
             model = "random")
```

```{r}
# POLS
pols <- plm(ln_Homicide ~ Inequality + Education_years + ln_GDP_per_capita +
              Lower_secondary_completion_rate + School_enrollment + Unemployment_int + Unsentenced + Police,
            data = df,
            index = c("Country", "Year"),
            model = "pooling")
```

## Model selecion

Here also about model_select() function

```{r}
source("functions/model_select.R")
```

<details><summary>Show the definition of the function</summary>

```{r}
model_select <- function(fixed, random, pols, sig.level = 0.05) {
  
  library(dplyr)
  library(plm)
  
  
  # Breusch-Pagan Lagrange Multiplier test for random effects
  random_effects <- plmtest(pols, type=c("bp"))
  
  random_effects_con <- ifelse(random_effects$p.value < sig.level,
                               "significant random effects",
                               "insignificant random effects")
  
  # F test for individual fixed effects
  fixed_effects <- pFtest(fixed, pols)
  
  fixed_effects_con <- ifelse(fixed_effects$p.value < sig.level,
                              "significant fixed effects",
                              "insignificant fixed effects")
  
  # Breusch-Pagan Lagrange Multiplier test for time-fixed effects
  time_fixed_effects <- plmtest(fixed, c("time"), type=("bp"))
  
  time_fixed_effects_con <- ifelse(time_fixed_effects$p.value < sig.level,
                                   "time-fixed effects needed",
                                   "no time-fixed effects needed")
  
  # Hausmann test
  hausmann <- phtest(fixed, random)
  
  hausmann_con <- ifelse(hausmann$p.value < sig.level,
                         "choose fixed effects model",
                         "choose random effects model")
  
  
  # Data frame result
  result <- data.frame(
    test = c("Breusch-Pagan LM test for random effects",
             "F test for individual fixed effects",
             "Breusch-Pagan LM test for time-fixed effects",
             "Hausmann test"
             ),
    p.value = c(random_effects$p.value, fixed_effects$p.value, 
              time_fixed_effects$p.value, hausmann$p.value) %>% 
      round(4),
    conclusion = c(random_effects_con, fixed_effects_con,
                 time_fixed_effects_con, hausmann_con) ,
    row.names = NULL
    )
  
  result$p.value <- ifelse(result$p.value == 0, "< 0.0001" , result$p.value)
  
  
  return(result)

}
```
</details>
<br/>

```{r, echo=F}
model_select(fixed, random, pols) %>% 
  kbl(booktabs = T) %>%
  kable_material_dark(full_width = F, bootstrap_options = c("hover"), font_size = 14)
```

## General-to-specific modeling {.tabset .tabset-fade .tabset-pills}

some info about it

### Base model

Let us take a look at the base model, which we called `fixed`.

<center>
```{r, echo=F, results = 'asis'}
stargazer(fixed, type = "html", single.row = T, report = "vcs*")
```
</center>

### Step 1

**Deleting *Lower_secondary_completion_rate* **

First, we generate the model without *Lower_secondary_completion_rate*.

```{r}
model1.1 <- plm(ln_Homicide ~ Inequality + Education_years + ln_GDP_per_capita +
                School_enrollment + Unemployment_int + Unsentenced + Police,
                data = df,
                index=c("Country", "Year"),
                model="within")
```

Next, we check the significance of omitting this variable using Wald test. Namely, we test whether the coefficient of the *Lower_secondary_completion_rate* variable is equal to zero.

```{r, echo=F}
h <- rbind(c(0,0,0,1,0,0,0,0,0))

wald.test(b = coef(fixed), Sigma = vcov(fixed), L = h)
```

We fail to reject the null hypothesis regarding the introduced constraint. Therefore, we can omit *Lower_secondary_completion_rate*. The model without this variable is as follows:

<center>
```{r, echo=F, results = 'asis'}
stargazer(model1.1, type = "html", single.row = T, report = "vcs*p")
```
</center>


### Step 2

**Deleting *Police* **

First, we generate the model without *Police* and without *Lower_secondary_completion_rate*.

```{r}
model1.2 <- plm(ln_Homicide ~ Inequality + Education_years + ln_GDP_per_capita +
                School_enrollment + Unemployment_int + Unsentenced,
                data = df,
                index=c("Country", "Year"),
                model="within")
```

Next, we check the significance of omitting these variables using Wald test. Namely, we test whether the coefficients of *Police* and *Lower_secondary_completion_rate* are both equal to zero.

```{r, echo=F}
h <- rbind(c(0,0,0,1,0,0,0,0,0),c(0,0,0,0,0,0,0,0,1)) 

wald.test(b = coef(fixed), Sigma = vcov(fixed), L = h)
```

We fail to reject the null hypothesis regarding the introduced constraints. Therefore, we can also omit *Police*. The model without these variables is as following:

<center>
```{r, echo=F, results = 'asis'}
stargazer(model1.2, type = "html", single.row = T, report = "vcs*p")
```
</center>


### Step 3

**Deleting *Inequality* **

First, we generate the model without *Inequality*, *Police* and *Lower_secondary_completion_rate*.

```{r}
model1.3 <- plm(ln_Homicide ~ Education_years + ln_GDP_per_capita +
                School_enrollment + Unemployment_int + Unsentenced,
                data = df,
                index=c("Country", "Year"),
                model="within")
```

Next, we check the significance of omitting these variables using Wald test. Namely, we test whether the coefficients of *Inequality*, *Police* and *Lower_secondary_completion_rate* are all equal to zero.

```{r, echo=F}
h <- rbind(c(0,0,0,1,0,0,0,0,0),c(0,0,0,0,0,0,0,0,1),c(1,0,0,0,0,0,0,0,0))

wald.test(b = coef(fixed), Sigma = vcov(fixed), L = h)
```

We fail to reject the null hypothesis regarding the introduced constraints. Therefore, we can also omit *Inequality*, which contradicts literature. The model without these variables is as follows:

<center>
```{r, echo=F, results = 'asis'}
stargazer(model1.3, type = "html", single.row = T, report = "vcs*p")
```
</center>

### Final model

All the remaining variables are significant and jointly significant, so our final model is as follows:

<center>
```{r, echo=F, results = 'asis'}
stargazer(model1.3, type = "html", single.row = T, report = "vcs*")
```
</center>

---

## Model diagnostic

What is going on and why important.. what is this function

```{r}
source("functions/model_diagnostic.R")
```

<details><summary>Show the definition of the function</summary>

```{r}
model_diagnostic <- function(model, sig.level = 0.05) {
  
  library(dplyr)
  library(lmtest)
  library(plm)
  library(tseries)
  
  
  # Jarque-Bera LM test for normality of residuals
  jarque_bera <- jarque.bera.test(model$residuals)
  
  jarque_bera_con <- ifelse(jarque_bera$p.value < sig.level,
                            "not normally distributed residuals", "normally distributed residuals")
  
  # Breusch-Pagan LM test for cross-sectional dependence
  cross_sectional_BP <- pcdtest(model, test = c("lm"))
  
  cross_sectional_BP_con <- ifelse(cross_sectional_BP$p.value < sig.level,
                                   "cross-sectional dependence", "no cross-sectional dependence")
  
  # Pesaran CD test for cross-sectional dependence
  cross_sectional_P <- pcdtest(model, test = c("cd"))
  
  cross_sectional_P_con <- ifelse(cross_sectional_P$p.value < sig.level,
                                  "cross-sectional dependence", "no cross-sectional dependence")
  
  # Breusch-Godfrey/Wooldridge test for serial correlation
  serial_correlation <- pbgtest(model)
  
  serial_correlation_con <- ifelse(serial_correlation$p.value < sig.level,
                                   "serial correlation", "no serial correlation")
  
  # Breusch-Pagan test for heteroskedasticity
  heteroskedasticity <- bptest(model$formula,
                               data = model$model,
                               studentize=F)
  
  heteroskedasticity_con <- ifelse(heteroskedasticity$p.value < sig.level,
                                   "heteroskedasticity", "homoskedasticity")
  
  
  # Data frame result
  result <- data.frame(
    test = c("Jarque-Bera LM test for normality of residuals",
             "Breusch-Pagan LM test for cross-sectional dependence",
             "Pesaran CD test for cross-sectional dependence",
             "Breusch-Godfrey/Wooldridge test for serial correlation",
             "Breusch-Pagan test for heteroskedasticity"
    ),
    p.value = c(jarque_bera$p.value, cross_sectional_BP$p.value, cross_sectional_P$p.value, 
                serial_correlation$p.value, heteroskedasticity$p.value) %>% 
      round(4),
    conclusion = c(jarque_bera_con, cross_sectional_BP_con, cross_sectional_P_con,
                   serial_correlation_con, heteroskedasticity_con),
    row.names = NULL
  )
  
  result$p.value <- ifelse(result$p.value == 0, "< 0.0001" , result$p.value)
  
  
  return(result)
  
}
```
</details>
<br/>

```{r, echo=F}
model_diagnostic(model1.3) %>% 
  kbl(booktabs = T) %>%
  kable_material_dark(full_width = F, bootstrap_options = c("hover"), font_size = 14)
```

```{r, echo=F, fig.align='center', out.width = '80%'}
hist(model1.3$residuals,
     density = 20, breaks = 20, prob = T, xlim = c(-1,1), 
     xlab="Residuals",
     main="Reproduction model - histogram of the residuals")
curve(dnorm(x,
            mean = mean(model1.3$residuals),
            sd = sqrt(var(model1.3$residuals))), 
      col="steelblue", lwd = 2, add = T, yaxt = "n")
```

### Robust estimator

Info about it, why this and why it is good

```{r, eval=F}
coeftest(model1.3,
         vcovHC(model1.3, method = "arellano", type="HC0", cluster = "time"))
```

---

## BA model

Let us now estimate the model that has been recognized as final in the BA thesis to which we are referring. For this purpose, we will use exactly those independent variables that were present in that BA model.

```{r}
model_BA <- plm(ln_Homicide ~ Inequality + ln_GDP_per_capita + School_enrollment + Police + Unsentenced,
                data = df, 
                index=c("Country", "Year"),
                model="within")
```

<center>
```{r, echo=F, results = 'asis'}
stargazer(model_BA, type = "html", single.row = T, report = "vcs*p")
```
</center>

```{r, echo=F}
model_diagnostic(model_BA) %>% 
  kbl(booktabs = T) %>%
  kable_material_dark(full_width = F, bootstrap_options = c("hover"), font_size = 14)
```

Not all variables are significant in the model constructed in this way.


<center>
```{r, echo=F, results = 'asis'}
model1.final <- coeftest(model1.3, vcovHC(model1.3, method = "arellano", type="HC0", cluster = "time"))

model_BA.final <- coeftest(model_BA, vcovHC(model_BA, method = "arellano", type="HC0", cluster = "time"))

stargazer(model_BA.final, model1.final,
          type = "html", column.labels = c("Bachelor","Reproduction"),
          title = "BA model vs. reproduction model", column.sep.width = "10px")

```
</center>

***